<!DOCTYPE html>
<html lang="ko">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Document</title>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs/dist/tf.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/posenet@2.2.2/dist/posenet.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-converter@3.11.0/dist/tf-converter.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/pose-detection@0.0.6/dist/pose-detection.min.js"></script>
    <style>
      body {
        height: 100%;
        width: 100%;
      }
      video,
      canvas {
        position: absolute;
      }
      #cap_result {
        position: absolute;
        top: 70%;
      }
    </style>
  </head>
  <body>
    <h1>PoseNet Training</h1>
    <h2 id="result_label"></h2>
    <div>
      <video
        src=""
        id="video"
        width="640"
        height="480"
        autoplay
        muted
        playsinline
      ></video>
      <canvas id="canvas"></canvas>
      <h3 id="cap_result"></h3>
    </div>
    <script>
      const video = document.getElementById("video");
      const canvas = document.getElementById("canvas");
      const cap_result = document.getElementById("cap_result");
      const context = canvas.getContext("2d");
      const result_label = document.getElementById("result_label");
      navigator.mediaDevices
        .getUserMedia({
          video: true,
          audio: false,
        })
        .then(function (stream) {
          video.srcObject = stream;
        });
      posenet.load().then((model) => {
        console.log(model);
        video.onloadeddata = (e) => {
          console.log("정상");
          predict();
          Capture();
        };
        function Capture() {
          model.estimateSinglePose(video).then((pose) => {
            console.log(pose.keypoints);
            pose.keypoints.forEach((v, i) => {
              cap_result.innerHTML = `${v.part}의 값은 ${v.score}`;
            });
          });
        }
        function predict() {
          model.estimateSinglePose(video).then((pose) => {
            canvas.width = video.width;
            canvas.height = video.height;
            drawKeypoints(pose.keypoints, 0.6, context);
            drawSkeleton(pose.keypoints, 0.6, context);
          });
          requestAnimationFrame(predict);
        }
      });
      // 기본예제
      const color = "blue";
      const boundingBoxColor = "red";
      const lineWidth = 2;
      function toTuple({ y, x }) {
        return [y, x];
      }

      function drawPoint(ctx, y, x, r, color) {
        ctx.beginPath();
        ctx.arc(x, y, r, 0, 2 * Math.PI);
        ctx.fillStyle = color;
        ctx.fill();
      }

      function drawSegment([ay, ax], [by, bx], color, scale, ctx) {
        ctx.beginPath();
        ctx.moveTo(ax * scale, ay * scale);
        ctx.lineTo(bx * scale, by * scale);
        ctx.lineWidth = lineWidth;
        ctx.strokeStyle = color;
        ctx.stroke();
      }

      function drawSkeleton(keypoints, minConfidence, ctx, scale = 1) {
        const adjacentKeyPoints = posenet.getAdjacentKeyPoints(
          keypoints,
          minConfidence
        );
        adjacentKeyPoints.forEach((keypoints) => {
          drawSegment(
            toTuple(keypoints[0].position),
            toTuple(keypoints[1].position),
            color,
            scale,
            ctx
          );
        });
      }

      function drawKeypoints(keypoints, minConfidence, ctx, scale = 1) {
        for (let i = 0; i < keypoints.length; i++) {
          const keypoint = keypoints[i];
          if (keypoint.score < minConfidence) {
            continue;
          }
          const { y, x } = keypoint.position;
          drawPoint(ctx, y * scale, x * scale, 3, color);
        }
      }
    </script>
  </body>
</html>
